{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94b1b2d9-9305-4957-a773-c946ce6b17bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06b0f91f-4ebd-409a-92db-a34ea12ce68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/opt/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b78465e-52ec-4a18-bd2a-65f9a867de25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import date\n",
    "import holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd07b382-f362-42d4-8894-a32fc687153b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/01 11:55:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Spark ML Custom Transformation\").master(\"local[*]\") \\\n",
    ".config(\"spark.driver.memory\", \"4g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cdc26b1-4cf6-4c1e-9557-99e000d047fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- store: long (nullable = true)\n",
      " |-- item: long (nullable = true)\n",
      " |-- label: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('2013-01-01',1, 1, 13),('2013-01-02', 1, 1, 11), ('2013-01-03', 1, 1, 14), ('2013-01-04', 1, 1, 10), ('2013-01-05', 1, 1, 10) ]\n",
    "cols = ['date', 'store', 'item', 'label']\n",
    "\n",
    "df = spark.createDataFrame(data, cols).withColumn('date', F.to_date('date', 'yyy-MM-dd'))\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97151cbb-3017-47a1-8dec-1aa3b5699545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----+-----+\n",
      "|      date|store|item|label|\n",
      "+----------+-----+----+-----+\n",
      "|2013-01-01|    1|   1|   13|\n",
      "|2013-01-02|    1|   1|   11|\n",
      "|2013-01-03|    1|   1|   14|\n",
      "|2013-01-04|    1|   1|   10|\n",
      "|2013-01-05|    1|   1|   10|\n",
      "+----------+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bedc6f77-1cd7-4724-9d72-ecfb9322f5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Transformer\n",
    "from pyspark.sql.functions import lit, udf\n",
    "from pyspark.ml.param.shared import HasInputCols, HasOutputCol\n",
    "from datetime import date\n",
    "\n",
    "# @udf(IntegerType())\n",
    "# def is_holiday(date_str: date, country_code: str='TR'):\n",
    "#     date_str = str(date_str)\n",
    "#     country_holidays = holidays.CountryHoliday(country_code) \n",
    "#     date_obj = date.fromisoformat(date_str)\n",
    "#     if date_obj in country_holidays:\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return 0\n",
    "        \n",
    "class AddDateFeaturesTransformer(Transformer, HasInputCols, HasOutputCol):\n",
    "    def __init__(self, inputCol=None, outputCols=None, country_code=None):\n",
    "        super(AddDateFeaturesTransformer, self).__init__()\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCols = outputCols\n",
    "        self.country_code = country_code\n",
    "        \n",
    "    def is_holiday(self, date_str: date, country_code: str='TR'):\n",
    "        date_str = str(date_str)\n",
    "        country_holidays = holidays.CountryHoliday(country_code) \n",
    "        date_obj = date.fromisoformat(date_str)\n",
    "        if date_obj in country_holidays:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def _transform(self, df):\n",
    "        is_holiday = udf(self.is_holiday, IntegerType())\n",
    "        \n",
    "        df = df.withColumn(self.outputCols[0], F.year(self.inputCol)) \\\n",
    "        .withColumn(self.outputCols[1], F.month(self.inputCol)) \\\n",
    "        .withColumn(self.outputCols[2], F.dayofweek(self.inputCol)) \\\n",
    "        .withColumn(self.outputCols[3], is_holiday(self.inputCol))\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ff2edc8-e29c-4322-8375-813407f4eb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "sct = AddDateFeaturesTransformer(inputCol='date', outputCols=['y','m','d','is_holiday'], country_code='TR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fd050e2-1ca8-4030-9d96-1801a41c3dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:==============>                                            (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----+-----+----+---+---+----------+\n",
      "|      date|store|item|label|   y|  m|  d|is_holiday|\n",
      "+----------+-----+----+-----+----+---+---+----------+\n",
      "|2013-01-01|    1|   1|   13|2013|  1|  3|         1|\n",
      "|2013-01-02|    1|   1|   11|2013|  1|  4|         0|\n",
      "|2013-01-03|    1|   1|   14|2013|  1|  5|         0|\n",
      "|2013-01-04|    1|   1|   10|2013|  1|  6|         0|\n",
      "|2013-01-05|    1|   1|   10|2013|  1|  7|         0|\n",
      "+----------+-----+----+-----+----+---+---+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sct.transform(df).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb4a6bc-ee92-454f-ad55-d5d0bdb99e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
